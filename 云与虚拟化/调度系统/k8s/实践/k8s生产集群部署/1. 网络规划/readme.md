# 宗旨

本文旨在完成生产可用的k8s集群的网络规划并主要侧重流量切分，使得管理所需的流量与租户容器的流量以及访问存储和中间件的流量不会混杂在一起

# 前提与假设

* 当前规划是在1个可用域(region)内，跨可用域默认情况下应当认为会单独部署1套集群，而不是将本地集群的网络扩展到新的可用域
* 规划按照1个可用域有3个可用区(Availability Zone)执行

# 网络类型

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(办公大楼, 办公网, 10.0.0.0/16) {
}

Boundary(生产机房, 生产机房) {
    Boundary(外部网络, 外部网络, 10.99.1.0/24)
    Boundary(业务网络1, 业务网络, 10.100.0.0/24)
    Boundary(业务网络2, 业务网络, 10.100.1.0/24)
    Boundary(业务网络3, 业务网络, 10.100.2.0/24)
    Boundary(业务网络4, 业务网络, 172.16.0.0/24)
    Boundary(云底层网络, 云底层网络, 100.64.0.0/17) {
        Boundary(云管理网络, 云管理网络, 100.64.0.0/19)
        Boundary(容器流量网络, 容器流量网络, 100.64.32.0/19) {
            Boundary(pod网络, pod网络, 10.240.0.0/16) #orange
            Boundary(k8s集群网络, k8s集群网络, 10.254.0.0/16) #orange
        }
        Boundary(存储访问网络, 存储访问网络, 100.64.64.0/19)
    }
    外部网络-d->业务网络1
    外部网络-d->业务网络2
    外部网络-d->业务网络3
    外部网络-d->业务网络4
    外部网络-d-> 云底层网络
@enduml
```

在一个机房内部会很多张网，基本可以划分为

* 办公网: 员工所在的网络
* 外部网络: 通过这个网络能够到达公网或从公网能够访问
* 业务网络: 各种各样生产系统所在的网络以及企业内部办公系统所在的网络(OA、ERP等)
* 云底层网络: 运行云的基础设施和辅助组件的网络(云管理网络)
  ，pod的东西南北向流量(容器流量网络)以及云的底层宿主机与pod访问数据库，缓存，块存储等数据系统的流量(存储访问网络)
* pod网络: pod访问的层叠网络
* k8s集群网络: service运行的层叠网络

每一个网络如果用云的概念来表达，都可以被理解为一个vpc，这些vpc之间如果希望进行对等连接并达到互相访问的目标，则不能允许有重复ip地址段的出现。特别是如果pod想要直接访问业务系统的一个ip而不通过中间代理。
否则重叠的网段之间就需要第三个网络进行中转。

例如: pod想要访问某个业务主机，但是这两个主机的ip如果都是10.0.0.5，那无论如何两者都没有办法通过ip地址的方式直接通信。

网络中转较为通用的方式有

* 用类似挂载浮动ip的方式将需要访问的ip在一个与客户端ip无冲突且可以访问的网络中暴露(SNAT + DNAT)。
  这种方式的缺陷也比较明显，ip的映射是1对1的，如果希望有一个ip代理多台多个主机，则可以使用类似lvs/nginx的代理服务器的方式完成
* 代理服务器的缺陷是需要主机具备多网卡，如果代理服务器不具有多网卡，最终还是要从所在网络的网关将代理服务器的ip映射出去，然后再将这个地址对外暴露。

整体上，容器云的部署另机房多了5张网: 云底层的管理网、容器流量网(也成流量网或业务网)、存储访问网(也称存储网)，
pod运行的pod网络和service运行的集群网络

# ip地址规划

在建设容器云之间，需要提前规划好底层设施以及pod和service使用的网络ip段，以免后续发生非预期的影响。
上文给出个5个网络都需要划分ip地址段

划分的原则是

* 管理网、流量网、存储网的cidr应当基本一致。
* pod网络的cidr应当考虑预期承载的工作负载数量
* 集群网络的cidr应当考虑预期承载的负载均衡等流量转发工作负载的数量

## 云底层网络ip地址规划

云底层网络包含了管理网、流量网、存储网，可以给一个17掩码的连续地址使用，然后再分为4份，将ip地址分配给这3个子网。余下1段当做备用ip地址池。
17位地址掩码使得1个region内的可用地址有30000+个。管理网、流量网、存储网每个段也有8000+个左右的ip，再按照3个az切分，每个az也保留有2000多个ip，足够宿主机的使用和扩容。
这样的一个段需要从企业的it部门申请亦或者可以使用**100.64.0.0/17**来获得1个region的地址，
在第二个region可以使用**100.64.128.0/17**并以此类推，从而保证2个region之间的云底层设施的网络还有ip互访的可能。
但如果企业it反对，则需要考虑可用的地址空间以及云底层基础设施的扩容空间，毕竟整个网络按照用途和可用区需要切分为16个子网，不建议最终子网的主机规模小于1个C类，
从而连续的网段**至少**为20位掩码。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml


Boundary(云底层网络, 云底层网络) {
    Boundary(管理网, 管理网) {    
        Boundary(az1, AZ-1)
        Boundary(az12, AZ-2)
        Boundary(az13, AZ-3)
        Boundary(保留1, 保留段)
    }
    Boundary(流量网, 流量网) {
        Boundary(az2, AZ-1)
        Boundary(az22, AZ-2)
        Boundary(az23, AZ-3)
        Boundary(保留2, 保留段)
    }
    Boundary(存储网, 存储网) {
        Boundary(az3, AZ-1)
        Boundary(az32, AZ-2)
        Boundary(az33, AZ-3)
        Boundary(保留3, 保留段)
    }
    Boundary(保留5, 保留段)
}
@enduml
```

如果机房规模降低为为2个，则以1个机房有1个完整C类段作为基准考虑，需要的连续空间会显著下降，变更为21位掩码。此时当然建议还是划分19位掩码使用。

## pod网络的ip地址规划

pod网络的ip地址规的依据是2个

* 是否希望pod能够和外部ip直接通信，从而获得ip地址的数字
* 希望承载的pod数量，从而获得掩码的数字

目前的默认选择是10.254.0.0/16。16位掩码意味着承载6万个pod，每个pod就算1C也要6万C，按照一台服务器64C计算需要1000台，因此基本够用。

## k8s集群网络的ip地址规划

k8s集群的service运行的网络，不要和pod的网段有重叠，可以取10网段靠后的一个ip地址空间，比如10.253.0.0/16

# 云底层网络的AZ扩展

管理网、流量网、存储网都按照AZ切分了子网，并要求这些子网3层之间能直接路由通信。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml


Boundary(流量网, 流量网) {
    Boundary(AZ1, AZ-1) {
        Container(设备1, 物理节点)
        Container(网关1, 网关)
        Container(专线1, 专线设备)
        设备1 <-u->网关1
        网关1 <-->专线1
    }
    Boundary(AZ2, AZ-2) {
        Container(设备2, 物理节点)
        Container(网关2, 网关)
        Container(专线2, 专线设备)
        设备2 <-u->网关2
        网关2 <-->专线2
    }
    专线1 <-->专线2
}
@enduml
```

上图示例了流量网跨机房通信的方案。基于这种3层互通的假设，云的任何底层软件设计上就不能有强依赖于2层广播可达的设计。原则上底层网络也不会在两个机房之间直接建设可达的2层广播域。
这种假设理论上不会影响pod在2个机房的动态分步，因为pod采取的是重建机制，在另一个节点上重建时ip地址会发生变动。此外，容器云底层普遍采取了给予ip封装的封装协议，比如vxlan或其它封装协议，
这个协议从sdn服务的网卡以3层数据包的形式发出，发送给指定的1台或者若干台设备，整个链路使用的是3层协议，因此3层只要可达就能通信。

**警告**: 这个假设的前提是SDN设备使用3层封装发送pod负载。

# 管理网络

_提示_: 假定整个容器云获得了100.64.0.0/17作为可用网段，于是管理网从中以19位掩码。

管理网络是k8s集群节点之间进行心跳检查，api调用，etcd之间数据传输等流量的网络。
除去k8s的master节点外，云管平台，监控系统等管理云底座的应用也部署在这里。这样他们之间彼此调用就可以不需要走网关了，而是通过2层网络直通，性能较好。
当然，当查询是跨AZ的时候还是仍需进行3层路由。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(管理网络, 管理网络, 100.64.0.0/19) {
    System(master, k8s master, k8s集群组件) #red
    System(worker, k8s worker, k8s集群组件) #red
    System(ingress, k8s ingress, k8s集群组件) #red
    System(dockerRepo, docker  repo)
    System(prometheus, prometheus)
    System(elk, elk)
    System(git, git)
    System(jenkins, jenkins)
    System(云管平台, 云管平台)
    System(数据库, 数据库)
    System(缓存, 缓存)
    System(消息队列, 消息队列)
    System(..., ...)
}
@enduml
```

该网络的主要流量构成为

* k8s集群与api server之间的通信
* etcd与api server之间的通信
* k8s集群与git、docker仓库等通信
* 监控、日志系统与k8s集群之间的通信
* 对云控制台的访问
* 告警系统等对互联网的访问

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(管理网络, 管理网络, 100.64.0.0/19) {
  Boundary(管理网络1, 100.64.0.0/21, AZ-1)
  Boundary(管理网络2, 100.64.8.0/21, AZ-2)
  Boundary(管理网络3, 100.64.16.0/21, AZ-3)
  Boundary(管理网络4, 保留段, 100.64.24.0/19)
}
@enduml
```

在1个AZ的子网内，建议

* k8s集群从开始向后拿地址(从100.64.0.0开始向后)
* 云管平台的应用和组件从后往前拿地址(比如AZ-1为100.64.7.254)
* AZ内不划分子网，保持子网内通信直接通过网卡点对点转发不经过路由，特别是监控采集系统的流量全都经过网关可能造成网关的很大压力

# 容器流量网络

容器流量网络的作用是给容器云的pod提供东西向和南北向网络通信，也就是单独给租户用的网络。这个网络理论上能够从互联网访问进来，并能够访问互联网。原则上来说，k8s容器流量网络只有k8s集群的节点。
这也是为什么说管理网的那一堆应用最好从后往前拿ip，这样可以使得k8s节点在容器流量网络中的ip和管理的ip在位数靠近，好记。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(容器流量网络, 容器流量网络, 100.64.32.0/19) {
    System(k8smaster, k8s master)
    System(k8swoker, k8s worker)
    System(k8ingress, k8s ingress)
}
@enduml
```

该网络的主要流量构成为

* pod和集群内外部的数据和调用流量

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(容器流量网, 容器流量网, 100.64.32.0/19) {
  Boundary(容器流量网1, 100.64.32.0/21, AZ-1)
  Boundary(容器流量网2, 100.64.40.0/21, AZ-2)
  Boundary(容器流量网3, 100.64.48.0/21, AZ-3)
  Boundary(容器流量网4, 保留段, 100.64.56.0/21)
}
@enduml
```

# 存储流量网络

该网络用于宿主机连接后台块存储设备、文件存储设备以及对象存储设备、数据库、缓存等给租户用的中间件。
该网络的存在有利于在访问流量峰值到达时，降低因后台数据访问而造成的容器网络数据拥塞。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(存储访问网络, 存储访问网络, 100.64.64.0/19) {
  Boundary(存储访问网络1, 100.64.64.0/21, AZ-1)
  Boundary(存储访问网络2, 100.64.72.0/21, AZ-2)
  Boundary(存储访问网络3, 100.64.80.0/21, AZ-3)
  Boundary(存储访问网络4, 保留段, 100.64.88.0/21)
}
@enduml
```

该网络的主要流量构成为

* 物理机对远程块存储设备的数据访问流量
* pod对外部对象存储，文件存储等存储设施的直接访问流量
* pod对外部数据库，缓存等中间件的访问流量

## 服务器网卡规划

由网络规划可以导出服务器需要3块网卡，1个千兆(管理流量)，2个万兆(容器流量网络和存储流量网络)，实际采购过程中按预期的调用峰值和历史观测值执行。
此外，网卡之间是否进行bond(mode=1(active-backup))、是否和交换机之间执行双上联从而通过交换机堆叠或链路聚合等达成硬件高可用或高性能，
视**财力**和**财力**(确信)而为。 在预算紧张的情况下，网卡的合并规则是存储流量网与容器流量网合并，但**绝对**不推荐3合1模式

# 流量划分

网卡连接好后，大部分情况下宿主机会使用正确的网卡来发送请求。
比如说k8s worker要连接iscsi target，而这个存储服务器接入了存储网(至少把访问接口暴露给了存储网)，其访问地址是存储网的ip地址，
于是worker使用存储网网卡连接这个地址进行访问。这是操作系统的底层能力，它在程序进行客户端访问时，会选择正确的网卡发送该请求。
作为服务端时，则需要指定应用程序绑定的ip来确保流量从正确的网卡进来。
因此原则上来说，这3个网彼此之间不需要路由。需要对外访问的服务，主要在正确的网络上布置了自己的访问点即可。

下面给一个测试过的例子

* 一台机器有2个网卡: 100.64.0.53 + 192.168.0.53
* 这台机器的nginx监听: 100.64.0.53(流量从一个网卡进)
* nginx代理192.168.0.105:8080

![nginx配置文件.png](nginx配置文件.png)

* 从100.64.0.52 `curl http://100.64.0.53`

![正常访问.png](正常访问.png)

因此，只要节点上的出口ip的地址对，程序写的时候没抽疯非要指定从绑定的ip出发，就应该自动能通

# 三网网关

管理网、存储网、流量网的网关**建议**配置在一个三层交换机上，这样有利于

* 三网内没有多网卡的机器彼此交换流量时依然能有正确的网关路径
* 三网的出网流量统一从当前交换设备配置SNAT规则和彼此路由的NOTRACK规则

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(AZ_1, AZ-1) {
   Boundary(外部网络, 外部网络, AZ-1){
    Container(x.x.x.x, x.x.x.x.)
  }
  System(网关, 网关)
  Boundary(管理网络, 100.64.0.0/21, AZ-1){
    Container(100.64.0.1, 100.64.0.1)
  }
  Boundary(容器流量网, 100.64.32.0/21, AZ-1) {
    Container(100.64.32.1, 100.64.32.1)
  }  
  Boundary(存储访问网络1, 100.64.64.0/21, AZ-1) {
    Container(100.64.64.1, 100.64.64.1)
  }
}
网关<-u-> x.x.x.x
网关<-d-> 100.64.0.1
网关<-d-> 100.64.32.1
网关<-d-> 100.64.64.1
@enduml
```

pod出网流量大的时候建议吧容器流量网的网关单独部署。
然后和管理网/存储网的统一网关互指下一跳。

# 入向流量

## 去往管理系统

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(管理网络1, 100.64.0.0/21, AZ-1){
    System(nginx, nginx, 负载均衡)
    System(云管平台, 云管平台)
    System(openApi, openApi)
    nginx <-d-> 云管平台
    nginx <-d-> openApi
}
Boundary(访问者网络, 公共网络) {
    Container(可访问ip, 可访问ip)
}
Boundary(互联网, 互联网) {
     System(F5, F5, 接入设备)
}
Boundary(其它业务网, 其它业务网) {
     System(客户端, 客户端)
}
nginx <-r->可访问ip
客户端 <-d->可访问ip
F5 <-d->可访问ip
@enduml
```

整体来说，管理网对外暴露的服务用nginx执行反向代理，其基于域名或者端口的模式进行内部转发。落地方式是

* 找一台具有多网卡的服务器，一根线接入管理网，一根接入一个公共网络
* 其在公共网络暴露一个ip，这个ip使得云外需要访问的主机可达

这使得流量由这个nginx在2个网络之间中转，而不是网关中转。
具体内部代理设备如何被云管理网外的设备访问，可以在具体实施时咨询机房管理员。

## 去往pod

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
  Boundary(容器流量网1, 100.64.32.0/21, AZ-1) {
    System(nginx, nginx, 负载均衡)
    System(ingress1, ingress)
    System(ingress2, ingress)
    System(ingress3, ingress)
    nginx <-d-> ingress1
    nginx <-d-> ingress2
    nginx <-d-> ingress3
}
Boundary(访问者网络, 公共网络) {
    Container(可访问ip, 可访问ip)
}
Boundary(互联网, 互联网) {
     System(F5, F5, 接入设备)
}
Boundary(其它业务网, 其它业务网) {
     System(客户端, 客户端)
}
nginx <-r->可访问ip
客户端 <-d->可访问ip
F5 <-d->可访问ip
@enduml
```

玩法一样

可见入网流量走nginx跨网转发，不推荐直接访问流量网内部地址导致走流量网网关，特别是如果三个网的网关还在一起的时候。

# 出向流量

## 出互联网

需要注意，出互联网的流量可以和入互联网的流量不是同一个ip，且基本不会一样，因为入网需要的带宽和出网不一样。
k8s集群节点的默认路由是容器流量网卡，需要由容器流量网的网关将数据或者路由或者SNAT到互联网出口。
管理网内非k8s主机的默认路由是管理网网关，同样需要管理网网关进行SNAT或路由到互联网出口。

## 去往机房内部其它网络内的系统

基于流量划分的目标，如果k8s集群节点要求访问机房内部的其它系统，则需要考虑该系统是存储系统还是控制系统，这两个系统需要在管理网/存储网暴露访问ip。
否则需要在主机上进行路由设置以保证访问使用正确的网卡进行，并在管理网/存储网网关上设置到对应网络的路径。
若访问的业务系统既不是管理用也不是存储用，那么可以理解为是pod对一个和自己业务相关的系统的流量，则或者该业务系统在容器流量网映射一个ip或者按照默认路由从容器流量网上到网关，
在网关处进行流量路由或SNAT转发。