# 宗旨

本文旨在完成生产可用的k8s集群的网络规划并主要侧重流量切分，使得管理所需的流量与租户容器的流量以及访问存储和中间件的流量不会混杂在一起

# 前提与假设

* 当前规划是在1个可用域(region)内，跨可用域默认情况下应当认为会单独部署1套集群，而不是将本地集群的网络扩展到新的可用域
* 规划按照1个可用域有3个可用区(Availability Zone)执行

# 网络类型与互联的方法

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(外部网络, 外部网络, 出入公网) {
    Container(互联网出口, 互联网出口)
}

Boundary(机房内其它网络, 机房内其它网络, 部署有应用、存储等) {
}

Boundary(云平台underlay网络, 云平台underlay网络, 内部网络) {
    Boundary(pod网络, pod网络, overlay)
    Boundary(cluster网络, cluster网络, overlay)
}

互联网出口<-->云平台underlay网络
机房内其它网络<-->云平台underlay网络
机房内其它网络<-l->互联网出口

@enduml
```

在一个机房内部会很多张网，基本可以划分为

* 外部网络: 通过这个网络能够到达公网或从公网能够访问
* 机房内其它网络: 各种各样生产系统所在的网络以及企业内部办公系统所在的网络(OA、ERP等)
* 云平台underlay网络: 运行云的基础设施和辅助组件的网络
* pod overlay网络: pod运行的层叠网络
* cluster overlay网络: service运行的层叠网络

每一个网络在对方眼中都是一个完全的内部网络，意味着网络和网络之间不会进行两两的路由打通，而是彼此通过SNAT和反向代理来访问指定的网段以及应用。
在实际落地上，SNAT通常用于支撑某个网络能够联通互联网。其它网络和网络之间的访问很难直接通过某个网络内的ip直接访问，理由是每个网络内都充斥着各种内网网段，很难做到被访问地址和自己内部的某个地址之间不重合。

# 互联访问的进一步思考

有关路由打通的问题，pod的overlay ip段默认是10网段。已知pod可以利用underlay网络和集群外通信。
这时如果要求pod访问机房内一个10.0.0.2的ip，但是恰好有pod的ip是10.0.0.2，对于pod而言，应当怎么访问呢？
它访问的到底是什么？

因此各个网络之间绝对不会使用路由直接通信，一定是将自己的服务选择在对方网络中暴露1个地址提供给对方访问。

两个网络之间直接暴露也不一定能解决问题，比如

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(机房网络A, 机房网络A, 192.168.0.0/24) {
}

Boundary(机房网络B, 机房网络B, 192.168.0.0/24) {
}

Boundary(云平台underlay网络, 云平台underlay网络, 192.168.0.0/24) {
}

@enduml
```

划网络的时候谁和谁可能也没商量过，搞不好3个网的ip段就冲突了。
这时候可能发生云平台underlay的一台宿主机访问机房网络A中的一台192.168..0.2暴露在自己网络中的ip192.168.0.200。
192.168.0.200在转发时肯定要转发回192.168..0.2，这时候它怎么确定192.168..0.2在underlay网络中没有被使用？
就算可以在iptables中指定转发网卡，但是整个规则配置起来也容易乱套。
因此，需要一个ip地址段和任何网络都不冲突的中间的公共网络来解决这种问题。
这个网络可以复用机房已有的成果，也可以独立划分。其ip地址段通常是100.64.0.0/10中的一段。
本文在此将这个网络称为云平台邻接网络

# 云平台邻接网络

云平台邻接网络提供一段公共的地址空间，它确保自己的地址空间和云平台的underlay网络以及常见的内网地址段之间都不冲突，且也不会机房内的任何一段地址冲突。
这段ip通常从100.64.0.0/10的运营商级别内网网段获取，在规划中原则上使用100.127.0.0/16(即最后一个可用网段)，取60000+个可用ip

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(外部网络, 外部网络, 出入公网) {
    Container(互联网出口, 互联网出口)
    Container(互联网入口, 互联网入口)
}

Boundary(机房内其它网络, 机房内其它网络, 部署有应用、存储等) {
}

Boundary(云平台邻接网, 云平台邻接网, 公共网络) {
}

Boundary(云平台underlay网络, 云平台underlay网络, 内部网络) {
}

互联网出口<-d->云平台underlay网络
机房内其它网络<-d->云平台邻接网
云平台underlay网络<-r->云平台邻接网
机房内其它网络<-u->互联网出口
互联网入口 <-d->云平台邻接网
@enduml
```

如果熟悉公有云的vpc概念，不难发现云的underlay网络，
以及pod和service的overlay网络还有机房内形形色色的网络都可以被理解为单独的vpc。
这些vpc之间彼此规划网段的时候无法保证不互相覆盖，因此不能通过路由直接链接。
如果某个vpc的服务想要对另一个vpc进行暴露，则通常的做法是这样

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
System(vpc1路由,vpc1-nat)
System(vpc2路由,vpc2-nat)
Boundary(vpc1, vpc-1, 192.168.0.0/24) {
    Container(192.168.0.100_1, 192.168.0.100, 客户端)
    Container(192.168.0.200_1, 192.168.0.200, vpc-2接入点) #red
    192.168.0.100_1 <-r-> 192.168.0.200_1
}

Boundary(云平台邻接网, 云平台邻接网, 公共网络) {
    Container(100.127.0.201, 100.127.0.201, vpc-1出口)
    Container(100.127.0.202, 100.127.0.202, vpc-2服务暴露ip) #red
    100.127.0.201<-r->100.127.0.202
}

Boundary(vpc2, vpc-2, 192.168.0.0/24) {
    Container(192.168.0.100_2, 192.168.0.100, 服务端) #red
    Container(192.168.0.200_2, 192.168.0.200, 接入点) #red
}

vpc1路由 <-u-> 192.168.0.200_1
vpc1路由 <-u-> 100.127.0.201
vpc2路由 <-u-> 192.168.0.200_2
vpc2路由 <-u-> 100.127.0.202
192.168.0.100_2<-l->192.168.0.200_2
@enduml
```

假设vpc-1的192.168.0.100:63333想要访问vpc-2的192.168.0.100:80

vpc-1上的nat规则

```shell
iptables -t nat -A PREROUTING -s 192.168.0.0/24 -d 192.168.0.200 -j DNAT 100.127.0.202
iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -d 100.127.0.202 -j SNAT --to-source 100.127.0.201
iptables -t nat -A POSTROUTING -s 100.127.0.202 -d 192.168.0.0/24 -j SNAT --to-source 192.168.0.200
```

同时snat的记录为

192.168.0.100:63333 -> 100.127.0.202:80 : 100.127.0.201:63333 -> 100.127.0.202:80
100.127.0.202:80 -> 192.168.0.100:63333 : 192.168.0.200:80 -> 192.168.0.100:63333

* 第1条snat的记录让202的80端口的回包被当作192.168.0.100的包
* 第2条snat的记录看似让100给200的发包当作发给100.127.0.202，但实际上因为首先在PREROUTING链已经定义了100-200的包进行目标地址转换，所以不会用到这个记录

vpc-2上的nat规则

```shell
iptables -t nat -A PREROUTING -s 100.127.0.0/x -d 100.127.0.202 -j DNAT 192.168.0.100
iptables -t nat -A POSTROUTING -s 100.127.0.0/x -d 192.168.0.100 -j SNAT 192.168.0.200
iptables -t nat -A POSTROUTING -s 192.168.0.100 -d 100.127.0.0/x -j SNAT --to-source 100.127.0.202
```

同时snat记录为

100.127.0.201:63333 -> 192.168.0.100:80 : 192.168.0.200:63333 -> 192.168.0.100:80
192.168.0.100:80 -> 100.127.0.201:63333 :  100.127.0.202:80 -> 100.127.0.201:63333

* 第1条snat记录让100给200的回包转成100给201的回包
* 第2条让201给202的回报看似要转成发给100的，然是dnat规则直接转了，因此这个规则也用不到

通过这样设置，实现了vpc-2的服务在vpc-1暴露。如果vpc-3也想要访问vpc-2的服务，则在vpc-3也建一个nat设备和vpc-2的202 ip进行转发即可。
vpc2的规则接收的是来自邻接网所有ip的请求而不只是100.127.0.201的。

于是网络和网络之间通过这种方法就能互通。

**提示**: 如果觉得nat很费脑子且容易出错，可以考虑用nginx代理nat设备。在这种方法下

* 暴露方安装一个nginx，一个网卡接vpc-2，一个网卡接邻接网
* 客户端的子网也装一个nginx，一个网卡连vpc-1，一个网卡连邻接网
* 配置客户端的nginx转发规则，从vpc-1的192.168.0.200入，最终转发到100.127.0.202，最后再代理192.168.0.100即可。网络清晰性高得多。

## 互联网到云内的流量示意图

互联网到云内的流量通常是要进入指k8s对外暴露的ingress的流量。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(外部网络, 外部网络, 出入公网) {
    Container(互联网入口, 互联网入口, x.x.x.x/192.168.13.100)
    Container(ingress地址1, 跨网流量代理, 192.168.13.200)
    互联网入口 <-l-> ingress地址1
}
System(nginx, nginx)
Boundary(云平台邻接网, 云平台邻接网, 公共网络) {
    Container(ingress地址2, 跨网流量代理, 100.127.0.200)
    Container(网关出口1, ingress代理, 100.127.0.201)
}
Boundary(云平台underlay网络, 云平台underlay网络, 内部网络) {
    Container(网关出口2, ingress代理, 100.64.0.2)
}

nginx <-u-> ingress地址1
nginx <-d-> ingress地址2
互联网入口 <-u->互联网入口ip
@enduml
```

* 从互联网入口的视角出发，它认为ingress在一个内部网络，它需要一个ingress能被它所在的子网访问到的ip。
* ingress的地址来自于云平台邻接网络中的一个nginx，这个nginx一端连接外部网络，一端连接邻接网
*

# 管理网络

管理网络是k8s集群以及相关辅助设施和服务彼此之间通信的网络

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(管理网络, 管理网络, vlan-a) {
    System(git, git)
    System(dockerRepo, docker  repo)
    System(k8smaster, k8s master)
    System(etcd, etcd)
    System(elk, elk)
    System(jenkins, jenkins)
    System(prometheus, prometheus)
    System(容器云运维平台, 容器云运维平台)
}
@enduml
```

该网络的主要流量构成为

* k8s集群与api server之间的通信
* etcd与api server之间的通信
* k8s集群与git、docker仓库等通信
* 监控、日志系统与k8s集群之间的通信
* 企业容器云运维和运营人员通过企业内网与容器云运维(营)平台间的通信

该网络**推荐**使用一个独立的vlan号从而避免受到其它网络的二层数据干扰，该网络中的若干子网如下

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(管理网, 管理网, 100.64.0.0/16) {
  System(管理网路由器,管理网路由器)
  Boundary(云管理系统, 云管理系统网段, 100.64.0.0/18)
  Boundary(k8s系统网段, k8s管理和控制系统网段, 100.64.64.0/18)
  Boundary(企业内网隔离子网, 企业内网应用映射区, 100.64.128.0/18)
  Boundary(预留地址, 预留地址, 100.64.192.0/18)
  Boundary(企业网, 企业网, 各种内部系统)
}

管理网路由器 -d-> 云管理系统
管理网路由器 -d-> k8s系统网段
管理网路由器 -d-> 企业内网隔离子网
管理网路由器 -u-> 企业网
@enduml
```

首先管理网将使用100.64.0.0/10这个运营商级保留地址的16位掩码，计划领走60000+个ip地址，
并假定会有3个可能的机房(3个可用区)来消耗这些地址。

这些地址的用途也主要分为3类

* 部署云的管理系统以及比如docker repo，git之类的系统使用的云管理系统网段
* 部署k8s集群，并主要支持管理和控制系统之间通信的k8s管理和控制系统网段
* 将云管理系统与企业内部打通使用的缓冲区，企业内网应用映射区

因此，整个地址池再划分为4个子池，3个分配给以上的内容，1个留作备用

## 云管理系统

云管理系统网段的用途是部署云管平台的系统应用和界面，包含运维管理员使用的控制台和租户使用的控制台，该子网计划分配16382个可用ip，
可直接部署16000+个云管理用服务，向下以20位掩码按照机房切分。可以且分为4段，前3段为3个机房地址池，第4段保留。
这样的好处是，每一个机房内部的系统请求在自己机房就消化了，只有跨子网的请求才需要走专线去另一个机房，同时二层广播也不会出机房边界
(3层的跨子网通信默认走网关，不会进行arp广播)，避免专线承担广播流量。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(管理网, 管理系统, 100.64.0.0/18) {
  Boundary(云管理系统1, 100.64.0.0/20, AZ-1)
  Boundary(云管理系统2, 100.64.16.0/20, AZ-2)
  Boundary(云管理系统3, 100.64.32.0/20, AZ-3)
  Boundary(云管理系统4, 保留段, 100.64.48.0/20)
}
@enduml
```

## k8s管理和控制系统网段

k8s系统网络部署有k8s的工作组件，包含master、worker等，同样有16382个ip，同样用20位掩码划分子网段，前3段作为机房内使用，第4段保留。

**不推荐**k8s管理和控制系统网段划子网，理由是master和worker之间有大量的api server的调用和状态上报，划了子网后流量各种都要经过网关没有意义。

同时，为了保证跨机房后k8s集群节点之间underlay网络能够进行二层广播，可能需要在多机房之间实现大二层网络。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(k8s, k8s管理和控制系统网段, 100.64.64.0/18) {
  Boundary(k8s系统网段1, 100.64.64.0/20, AZ-1)
  Boundary(k8s系统网段2, 100.64.80.0/20, AZ-2)
  Boundary(k8s系统网段3, 100.64.96.0/20, AZ-3)
  Boundary(k8s系统网段4, 保留段, 100.64.111.0/20)
}
@enduml
```

## 企业内网应用映射区

在一个机房内，云管理系统和k8s都可能要求访问企业内部的系统(比如发出告警)或者被企业内部的人员/系统访问
(比如运维通过企业内网登录运维控制台)。
面对这种需求，需要一个中间区域来代理业务网的内部应用。理由是企业内部的系统实在太多，每一个都在管理网直接用企业内部ip访问

* 企业内部ip可能和集群内的若干网络重合，造成无法访问
* 路由设置和nat设置过于复杂

因此管理网访问企业内部应用需要有1个或者几个稳定的访问入口。

和其他网络一样，按20位掩码切分。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(企业内网应用映射区, 企业内网应用映射区, 100.64.128.0/18) {
  Boundary(企业内网应用映射区1, 100.64.128.0/20, AZ-1)
  Boundary(企业内网应用映射区2, 100.64.144.0/20, AZ-2)
  Boundary(企业内网应用映射区3, 100.64.160.0/20, AZ-3)
  Boundary(企业内网应用映射区4, 保留段, 100.64.176.0/20)
}
@enduml
```

下面给出一个云管平台连接企业内部单点登录用token换用户信息的例子:

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(管理网, 管理网, 100.64.0.0/16) {
  System(管理网路由器,管理网路由器)
  Boundary(云管理系统, 云管理系统网段, 100.64.0.0/18) {
    System(云管控制台, 云管控制台, 100.64.0.2)
  }
  Boundary(企业内网隔离子网, 企业内网应用映射区, 100.64.128.0/18) {
    Container(单点登录映射地址, 单点登录映射地址, 100.64.128.2)
  }
  Boundary(企业网, 企业网, 各种内部系统) {
    System(单点登录, 单点登录, 10.0.0.2)
    Container(负载均衡网卡, 负载均衡网卡, 10.0.0.3)
  }
  
  云管控制台 -r-> 管理网路由器: 路由查找
  管理网路由器 -d-> 单点登录映射地址: 包转发
  单点登录映射地址 <-l-> 负载均衡网卡
    负载均衡网卡 <-->单点登录
}

@enduml
```

这是一个非常清晰的网络，企业内网应用有一个较为统一的负载均衡入口，被称为出向应用网关，该网关在映射区和企业网均有ip地址，使用反向代理技术让云管平台通过访问100.64.128.2获得访问单点登录系统的功能。
类似的做法可用于管理网内部的应用向企业网开放，建设多网卡LB作为应用代理是一种简单易懂且使用的方案。

# 容器流量网络

容器流量网络的作用是给容器云的pod提供东西向和南北向网络通信，也就是单独给租户用的网络。这个网络理论上能够从互联网访问进来，并能够访问互联网。

流量网计划从100.65.0.0/16领走60000+个地址。原则上来说，k8s容器流量网络只有k8s集群的节点。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(容器流量网络, 容器流量网络, vlan-b) {
    System(k8smaster, k8s master)
    System(k8swoker, k8s worker)
    System(k8ingress, k8s ingress)
}
@enduml
```

该网络的主要流量构成为

* pod和集群内外部的数据和调用流量

该网络**推荐**使用一个独立的vlan号从而避免受到其它网络的二层数据干扰。

由于网络内主要是k8s的节点，因此网络内部不设子网，只是按照20位地址进行划分

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(容器流量网, 容器流量网, 100.65.0.0/16) {
  Boundary(容器流量网1, 100.65.0.0/20, AZ-1)
  Boundary(容器流量网2, 100.65.16.0/20, AZ-2)
  Boundary(容器流量网3, 100.65.32.0/20, AZ-3)
  Boundary(容器流量网4, 保留段, 其它)
}
@enduml
```

下面的讲解以AZ-1的100.65.0.0/20网段为例，该网段为AZ-1机房内部使用。

## 解决pod之间的东西向流量

pod之间的东西向流量由容器网络的flannel(或其它cni)网卡发出，其地址是100.65.0.0/20段内的其它具备容器网网卡的主机。
因此东西向能够通信。

## 解决pod的出网流量

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Boundary(外部网络, 外部网络, 10.0.0.0/24) {
    Container(10.0.0.8, 10.0.0.8)
}
System(路由器, 路由器, SNAT)
Boundary(容器流量网, 100.65.0.0/20, AZ-1) {
    Container(100.65.0.1, 100.65.0.1)
}

路由器<-u->10.0.0.8
路由器<-d->100.65.0.1

@enduml
```

要是的容器流量网内的pod能出公网，方案是使用一个路由器充当容器流量网络的网关。在网关上配置SNAT规则，将出网流量转发。

## 解决pod的ingress入口流量

pod原则上通过ingress对外暴露7层应用流量

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Person(用户, 用户)
Boundary(外部网络, 外部网络, 10.0.0.0/24) {
    System(F5, F5)
    Container(10.0.0.8, 10.0.0.8)
    Container(10.0.0.9, 10.0.0.9)
}
System(路由器, 路由器, SNAT)
System(集群应用访问网关, 集群应用访问网关, nginx)
Boundary(容器流量网, 100.65.0.0/20, AZ-1) {
    Container(100.65.0.1, 100.65.0.1)
    Container(100.65.0.2, 100.65.0.2)
    Container(100.65.0.3, 100.65.0.3, ingress)
    Container(100.65.0.4, 100.65.0.4, ingress)
    Container(100.65.0.5, 100.65.0.5, ingress)
}

路由器<-u->10.0.0.8
路由器<-d->100.65.0.1

集群应用访问网关<-u->10.0.0.9
集群应用访问网关<-d->100.65.0.2

100.65.0.2<-d->100.65.0.3
100.65.0.2<-d->100.65.0.4
100.65.0.2<-d->100.65.0.5

用户<-->F5
F5 <-->10.0.0.9

@enduml
```

容器网络的一个主要作用是支持外部和内部的南北向流量，由于两个子网的南北向流量都要经过默认网关(上图的容器网路由器)，
因此南北向流量的DNAT要配置在容器网路由器上，出网的SNAT则按照路由的默认路径走向整个机房的出口。拓扑图示例如下

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml

Person(用户, 用户)
Boundary(机房网络, 机房网络) {
  System(入口ip, 入口ip)
  System(出网ip, 出网ip)
  System(安全防护设备,安全防护设备)
  System(前端负载均衡, 前端负载均衡, 10.0.x.100)
  Container(10.0.x.2, 10.0.x.2)
  Container(10.0.x.3, 10.0.x.3)
  Container(10.0.x.4, 10.0.x.4)
  
  Container(10.0.x.200, 10.0.x.200)
  System(容器网路由器,容器网路由器)
  Boundary(master子网, master 子网, 100.64.16.0/24)
  Boundary(worker子网, worker 子网, 100.64.17.0/24) {
    Container(100.64.17.2, 100.64.17.2, ingress)
    Container(100.64.17.3, 100.64.17.3, ingress)
    Container(100.64.17.4, 100.64.17.4, ingress)
  }
  用户 <-d->入口ip
  入口ip <-r-> 安全防护设备
  10.0.x.200<-r->出网ip
  安全防护设备 <-r-> 前端负载均衡
  前端负载均衡 <-d-> 10.0.x.2
  前端负载均衡 <-d-> 10.0.x.3
  前端负载均衡 <-d-> 10.0.x.4
  10.0.x.2 <-d-> 容器网路由器: dnat -> 100.64.17.2
  10.0.x.3 <-d-> 容器网路由器: dnat -> 100.64.17.3
  10.0.x.4 <-d-> 容器网路由器: dnat -> 100.64.17.4
  10.0.x.200 <-d-> 容器网路由器
  容器网路由器<-d-> 100.64.17.2
  容器网路由器<-d-> 100.64.17.3
  容器网路由器<-d-> 100.64.17.4
  容器网路由器<-d->master子网
}
@enduml
```

按照上图的样例, 在容器路网路由器配置DNAT规则到3台ingress

* dnat: src == 10.0.x.100 && dst == 10.0.x.2: 10.0.x.2 -> 100.64.17.2
* dnat: src == 10.0.x.100 && dst == 10.0.x.3: 10.0.x.2 -> 100.64.17.3
* dnat: src == 10.0.x.100 && dst == 10.0.x.4: 10.0.x.2 -> 100.64.17.4

同时配置SNAT规则(按照路由设置，3台ingress的出网流量应当经过容器网路由器)，因为负载均衡的转发，使得ingress看到的访问ip是10.0.x.100

* snat: src == 100.64.17.2 && dst == 10.0.x.100: 100.64.17.2 -> 10.0.x.2
* snat: src == 100.64.17.3 && dst == 10.0.x.100: 100.64.17.3 -> 10.0.x.3
* snat: src == 100.64.17.4 && dst == 10.0.x.100: 100.64.17.4 -> 10.0.x.4

为了100.64网段的机器能够出互联网，配置整体SNAT规则

* snat: src == 100.64.16.0/24 && dst !=100.64.0.0/10 -> 10.0.x.200
* snat: src == 100.64.17.0/24 && dst !=100.64.0.0/10 -> 10.0.x.200

以上规则将100.64之间的网络转发屏蔽，以免worker子网与master子网之间的通信被snat干扰
(未来还有容器网络和管理网络以及存储网络通信的需求)

**警告**: 整体出网SNAT规则的顺序要低于ingress向前段反向代理进行snat的规则；

# 存储流量网络

该网络用于宿主机连接后台块存储设备、文件存储设备以及对象存储设备、数据库、缓存等中间件。
该网络的存在有利于在访问流量峰值到达时，降低因后台数据访问而造成的容器网络数据拥塞。

该网络的主要流量构成为

* 物理机对远程块存储设备的数据访问流量
* pod对外部对象存储，文件存储等存储设施的直接访问流量
* pod对外部数据库，缓存等中间件的访问流量

该网络**推荐**使用一个独立的vlan号从而避免受到其它网络的二层数据干扰

## 服务器网卡规划

由网络规划可以导出服务器需要3块网卡，1个千兆(管理流量)，2个万兆(容器流量网络和存储流量网络)，实际采购过程中按预期的调用峰值和历史观测值执行。
此外，网卡之间是否进行bond(mode=1(active-backup))、是否和交换机之间执行双上联从而通过交换机堆叠或链路聚合等达成硬件高可用或高性能，
视**财力**和**财力**(确信)而为。 在预算紧张的情况下，网卡的合并规则是存储流量网与容器流量网合并，但**绝对**不推荐3合1模式