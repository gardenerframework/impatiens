# 宗旨

本文在于指导容器云集群的硬件规划和使用，主要将涉及不同用途的服务器选择以及网络设备和存储集群的选择。

# 总则

若无特殊说明，以下内容适用于所有服务器

* 基本以x86作为主要的cpu架构
* 操作系统磁盘需要进行raid-1

# 云管平台相关硬件

## 计算服务器

应当注意到，运管平台需要部署非常多的管理应用，而且每个应用似乎都无法有效利用一整个服务器硬件。
云管平台主要包含租户的控制台和运维管理员的控制台以及类似prometheus、alert manager等管理侧应用。

**计划上**，云管平台以及相关组件由一套单独的k8s集群支撑。这套k8s集群和要实际管理的容器云集群不是一套，而是独立开的系统。
使用k8s管理云管平台以及相关组件具备以下明显优势

* 避免管理平面的硬件故障等造成的管理功能失效，其中最严重的就是prometheus等监控告警系统的失效。
* 当管理面任务加剧时，可以通过水平扩展管理面服务器的方式解决。扩容后，k8s集群会自动进行应用平衡。
* 单region进行AZ水平延伸时，管理面的水平延伸和容器云的水平延伸可用类似的方法解决
* 管理面应用可以基于不同的团队被分配给不同的namespace，从而被分配合理的配额。

```plantuml
@startuml
!include  https://plantuml.s3.cn-north-1.jdcloud-oss.com/C4_Container.puml
Boundary(存储网, 存储网) {
    System(远程存储, 远程存储)
}
Boundary(管理网, 管理网, AZ-1) {
    Boundary(管理面服务器1, 管理面服务器, k8s) {
        Container(管理网网卡1, 管理网网卡)
        Container(存储网网卡1, 存储网网卡)
    }
    Boundary(管理面服务器2, 管理面服务器, k8s) {
        Container(管理网网卡2, 管理网网卡)
        Container(存储网网卡2, 存储网网卡)
    }
    Boundary(管理面服务器3, 管理面服务器, k8s) {
        Container(管理网网卡3, 管理网网卡)
        Container(存储网网卡3, 存储网网卡)
    }
    管理网网卡1<-r->管理网网卡2
    管理网网卡2<-r->管理网网卡3
    存储网网卡1 -d-> 远程存储
    存储网网卡2 -d-> 远程存储
    存储网网卡3 -d-> 远程存储
}
@enduml
```

在这样的设计下，管理面服务器

* cpu:mem为1:2
* 操作系统盘240g以上，用于存储安装的各种组件和管理面镜像
* 网卡推荐为2块，一块用于管理网 + k8s集群本身的pod流量；另一块用于访问存储设备
* 网卡也能为1块，这样所有网络流量就混杂在一起，而且可能要求管理网的网关承担较多的流量(比如去往存储网的)
* 数据盘看是否有可对接的后台磁盘服务器，推荐连接远程磁盘服务器，这样可以不需要采购额外的数据盘
* 若管理面服务器自己组成ceph存储集群向管理面提供块存储能力，则服务器的数据磁盘规格服从ceph存储的厂家的要求

## 存储系统

若要采购远程存储设备，**推荐**优先考虑容器云所需的存储容量，之后在存储网内的存储设施划分出管理面所需的磁盘池。

给管理面的磁盘存储主要的用途是

* 为管理面的主机按需分配数据盘块存储
* 按需的意思是：管理面主机有物理层写入大量数据的需求，比如管理面k8s的etcd存储
* 其它形式运行的pod看实际情况访问本地磁盘(无法迁移)。或者通过pvc从远程存储申领一块磁盘或目录空间。
* 原则上来说，还是希望管理面的pod如果访问磁盘，应当访问映射在物理机上的目录，并听过调度标记将pod只调度到对应的物理机上访问。

## 网络交换机

云管平台使用的网络交换机没有什么特别的要求，常规的千兆交换机即可。